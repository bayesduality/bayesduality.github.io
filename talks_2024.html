<!DOCTYPE html>
<html lang="en">
<head>
<title>The 2nd Bayes-Duality Workshop 2024</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://www.w3schools.com/lib/w3-theme-indigo.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
<link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css">
<link rel="stylesheet" href="file.css">
</head>
<body bgcolor="#E5E4E2">

  <div class="w3-padding">
   <h1 class="w3-xxlarge" style="text-align: center">Detailed Program (Under Construction)</h2>
  </div>
  <div class="w3-row">
        <div class="w3-container w3-padding w3-auto" style="width:100%">

            <div id="June12" class="w3-card w3-padding w3-border w3-white w3-round-large">

               <h2>June 12</h2>
               <p>Morning Session (chair: Emtiyaz Khan)</p>

               <ul class="w3-ul">
                  <li id="intro">[10:00-10:10] Masashi Sugiyama: Introduction to AIP</li>

                  <li id="intro">[10:10-10:30] Emtiyaz Khan: Logistics</li> 

                  <li id="vincent">[10:10-11:10] Vincent Fortuin: Use Cases for Bayesian Deep Learning in the Age of Foundation Models 
                     <label for="vincent1" class="read-more-trigger"> Details</label> 
                     <input type="checkbox" class="read-more-state" id="vincent1">
                     <span class="read-more-wrap">
                        <span class="read-more-target">
                           <p style="text-align: justify">Abstract: Many researchers have pondered the same existential questions in this day and age: Is scale really all you need? Will the future of machine learning rely exclusively on foundation models? Should we all drop our current research agenda and work on the next large language model instead? In this talk, I will try to make the case that the answer to all these questions should be a convinced “no” and that now, maybe more than ever, should be the time to focus on fundamental questions in machine learning again. I will provide evidence for this by presenting three modern use cases of Bayesian deep learning in the areas of interpretable additive modeling, neural network sparsification, and subspace inference for fine-tuning. Together, these will show that the research field of Bayesian deep learning is very much alive and thriving and that its potential for valuable real-world impact is only just unfolding.</p>
                           <p style="text-align: justify">Bio: Vincent Fortuin is a tenure-track research group leader at Helmholtz AI in Munich, leading the group for Efficient Learning and Probabilistic Inference for Science (ELPIS). He is also junior faculty at the Technical University of Munich, a Fellow of the Konrad Zuse School for Reliable AI, affiliated with the Munich Center of Machine Learning, and a Branco Weiss Fellow. His research focuses on reliable and data-efficient AI approaches leveraging Bayesian deep learning, deep generative modeling, meta-learning, and PAC-Bayesian theory. Before that, he did his PhD in Machine Learning at ETH Zürich and was a Research Fellow at the University of Cambridge. He is a member and unit faculty of ELLIS, a regular reviewer and area chair for all major machine learning conferences, and a co-organizer of the Symposium on Advances in Approximate Bayesian Inference (AABI) and the ICBINB initiative.</p>
                        </span> 
                     </span>
                  </li>

                  <li id="juho">[11:30-12:30] Juho Lee: Toward scalable and generalizable Bayesian deep learning
                     <label for="juho1" class="read-more-trigger"> Details</label> 
                     <input type="checkbox" class="read-more-state" id="juho1">
                     <span class="read-more-wrap">
                        <span class="read-more-target">
                           <p style="text-align: justify">Abstract: In the era of large-scale foundation models, Bayesian deep learning remains an indispensable tool due to its capability to quantify uncertainty in a principled manner and adapt continuously to dynamic environments. However, the well-known challenges of Bayesian learning—difficulty in posterior inference, high cost of Bayesian model averaging (BMA), and selecting appropriate prior distributions—are even more pronounced in modern AI models. In this talk, I will present our recent work addressing these challenges. First, for more efficient posterior inference in Bayesian neural networks (BNNs), I will discuss how meta-learning can enhance the mixing of stochastic gradient MCMC (SGMCMC) algorithms for various BNNs. Second, I will introduce our newly developed algorithm that reduces the cost of BMA using diffusion-based distribution matching techniques. Finally, I will present our work on meta-learning stochastic processes, which can serve as priors for a range of downstream tasks.</p>
                           <p style="text-align: justify">Bio: Dr. Juho Lee is an associate professor at the Kim Jaechul Graduate School of AI Korea Advanced Institute of Science and Technology (KAIST). He earned his Ph.D. in Computer Science & Engineering from Pohang University of Science & Technology (POSTECH) and did his postdoc in the Computational Statistics & Machine Learning group at the University of Oxford, working with Professor François Caron. His research primarily focuses on Bayesian deep learning, with significant contributions to Bayesian nonparametrics, meta-learning, and generative modeling.</p>
                        </span> 
                     </span>
                  </li>

                  <li>[12:30-14:00] Lunch Break (On Your Own) </li>
               </ul>

               <p>Afternoon Session (chair: Vincent Fortuin)</p>

               <ul class="w3-ul">
                  <li id="tutorial1">[14:00-16:00] Tutorial by Emtiyaz Khan: Bayesian Learning Rule
                    <label for="tutorial1d" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="tutorial1d">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: TBA</p>
                          <p style="text-align: justify">Bio: TBA</p>
                       </span> 
                    </span>
                 </li>

                 <li>[16:00-16:30] Break</li>

                 <li id="matt">[16:30-17:30] Matt Jones: Talk TBA
                    <label for="matt1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="matt1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: TBA</p>
                          <p style="text-align: justify">Bio: TBA</p>
                       </span> 
                    </span>
                 </li>
               </ul>
               <p style="text-align: right">[<a href="workshop_2024.html#program">Back to Program</a>]</p>
            </div> 
        </div>
  </div>
  <div class="w3-row">
    <div class="w3-container w3-padding w3-auto" style="width:100%">

        <div id="June13" class="w3-card w3-padding w3-border w3-white w3-round-large">

           <h2>June 13</h2>
           <p>Morning Session (chair: Alexander Immer)</p>

           <ul class="w3-ul">
              <li id="haavard">[10:00-11:00] Haavard Rue: Talk TBA
                <label for="haavard1" class="read-more-trigger"> Details</label> 
                <input type="checkbox" class="read-more-state" id="haavard1">
                <span class="read-more-wrap">
                   <span class="read-more-target">
                      <p style="text-align: justify">Abstract: TBA</p>
                      <p style="text-align: justify">Bio: TBA</p>
                   </span> 
                </span>
              </li>
              
              <li>[11:00-11:30] Break</li>
              
              <li id="tutorial3">[11:30-12:30] Tutorial by Eugene Ndiaye on Conformal Prediction
               <label for="tutorial3d" class="read-more-trigger"> Details</label> 
               <input type="checkbox" class="read-more-state" id="tutorial3d">
               <span class="read-more-wrap">
                  <span class="read-more-target">
                     <p style="text-align: justify">Abstract: TBA</p>
                     <p style="text-align: justify">Bio: TBA</p>
                  </span> 
               </span>
               </li>
            <li>[12:30-14:00] Lunch Break (On Your Own) </li>
           </ul>

           <p>Afternoon Session (chair: Eugene Ndiaye)</p>

           <ul class="w3-ul">
            <li id="eugene">[14:00-15:00] Eugene Ndiaye: From Conformal Predictions to Confidence Regions
               <label for="eugene1" class="read-more-trigger"> Details</label> 
               <input type="checkbox" class="read-more-state" id="eugene1">
               <span class="read-more-wrap">
                  <span class="read-more-target">
                     <p style="text-align: justify">Abstract: Conformal prediction methodologies have significantly advanced the quantification of uncertainties in predictive models. Yet, the construction of confidence regions for model parameters presents a notable challenge, often necessitating stringent assumptions regarding data distribution or merely providing asymptotic guarantees. We introduce a novel approach termed CCR, which employs a combination of conformal prediction intervals for the model outputs to establish confidence regions for model parameters. We present coverage guarantees under minimal assumptions on noise and that is valid in finite sample regime. Our approach is applicable to both split conformal predictions and black-box methodologies including full or cross-conformal approaches. In the specific case of linear models, the derived confidence region manifests as the feasible set of a Mixed-Integer Linear Program (MILP), facilitating the deduction of confidence intervals for individual parameters and enabling robust optimization. We empirically compare CCR to recent advancements in challenging settings such as with heteroskedastic and non-Gaussian noise.</p>
                     <p style="text-align: justify">Bio: I am currently a researcher in the Machine Learning Group @Apple in Paris. I focus mainly on optimization and uncertainty quantification. I was previously a postdoctoral researcher both at Georgia Institute of Technology (USA) and Riken AIP (Japan). I hold a PhD in Applied Mathematics from University of Paris Saclay (France). My doctoral thesis focused on the design and analysis of faster and safer optimization algorithms for variable selection and hyperparameter calibration in high dimension.</p>
                  </span> 
               </span>
            </li>

             <li id="tutorial4">[15:00-16:00] Tutorial by Thomas Moellenhoff on Convex Duality
                <label for="tutorial4d" class="read-more-trigger"> Details</label> 
                <input type="checkbox" class="read-more-state" id="tutorial4d">
                <span class="read-more-wrap">
                   <span class="read-more-target">
                      <p style="text-align: justify">Abstract: TBA</p>
                      <p style="text-align: justify">Bio: TBA</p>
                   </span> 
                </span>
             </li>

             <li>[16:00-16:30] Break</li>
             <li id="alex">[16:30-17:30] Alexander Immer: Talk TBA
               <label for="alex1" class="read-more-trigger"> Details</label> 
               <input type="checkbox" class="read-more-state" id="alex1">
               <span class="read-more-wrap">
                  <span class="read-more-target">
                     <p style="text-align: justify">Abstract: TBA</p>
                     <p style="text-align: justify">Bio: TBA</p>
                  </span> 
               </span>
            </li>
             
           </ul>
           <p style="text-align: right">[<a href="workshop_2024.html#program">Go to Program</a>]</p>
        </div> 
    </div>
    <div class="w3-row">
        <div class="w3-container w3-padding w3-auto" style="width:100%">
    
            <div id="June14" class="w3-card w3-padding w3-border w3-white w3-round-large">
    
               <h2>June 14</h2>
               <p>Morning Session (chair: Haavard Rue)</p>
    
               <ul class="w3-ul">
                  <li id="frank">[10:00-11:00] Frank Nielsen: Some generalizations of Bregman divergences
                    <label for="frank1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="frank1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: In this talk, I shall present several generalizations of Bregman divergences for machine learning with algorithmic and geometric considerations.
                            In particular, I will describe duality structures and a few algorithms on Bregman manifolds, introduce the Bregman duo pseudo-divergences, and present a generalization of convexity which yields conformal Bregman divergences.</p>
                          <p style="text-align: justify">Bio:
                            Frank Nielsen  prepared his PhD on computational geometry (1996) at INRIA Sophia-Antipolis (France). He is a Senior Researcher and Fellow of Sony Computer Science Laboratories Inc. (Sony CSL, Tokyo) where he currently conducts research on Structures, Dynamics, and Geometric Computing for AI and Information Theory.  He serves the following journals: Information Geometry (Springer), Transactions on Information Theory (IEEE), and Entropy (MDPI). Frank Nielsen co-organizes with Frederic Barbaresco the biannual conference Geometric Science of Information.</p>
                       </span> 
                    </span>
                  </li>
                  
                  <li>[11:00-11:30] Break</li>
    
                  <li id="jonghyun">[11:30-12:00] Jonghyun Choi: Talk TBA 
                     <label for="jonghyun1" class="read-more-trigger"> Details</label> 
                     <input type="checkbox" class="read-more-state" id="jonghyun1">
                     <span class="read-more-wrap">
                        <span class="read-more-target">
                           <p style="text-align: justify">Abstract: TBA</p>
                           <p style="text-align: justify">Bio: TBA</p>
                        </span> 
                     </span>
                  </li>
    
                  <li>[12:30-14:00] Lunch Break (On Your Own) </li>
               </ul>
    
               <p>Afternoon Session</p> 
    
               <ul class="w3-ul">
                 <li id="tutorial5">[14:00-15:00] Tutorial by Martin Mundt on Continual Learning: Pillars of forgetting in continual updates and the road to lifelong learning
                    <label for="tutorial5d" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="tutorial5d">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: Machine learning studies the design of models and training algorithms in order to learn how to solve tasks from data. Whereas traditional machine learning concentrates on predefined training datasets, the renaissance of continual learning also takes into account that the world is constantly evolving. In this tutorial, I will focus on the challenge of catastrophic interference when attempting respective continual updates and summarize mechanisms to counteract it. To this end, I will survey three pillars of approaches, spanning the perspective from data, to optimization, and choice of model. Finally, I will highlight how the challenge of sequential updates relates to broader machine learning and which further elements are required on the road to true lifelong learning systems.</p>
                          <p style="text-align: justify">Bio: Martin is an independent research group leader at TU Darmstadt and hessian.AI, where he leads the Open World Lifelong Learning lab. He is also a board member at the non-profit ContinualAI, a core-organizer of Queer in AI, was the Diversity & Inclusion chair at AAAI-24 and currently serves as Review Process Chair for CoLLAs 2024.
                           Previously, he was an interim professor and postdoctoral researcher at TU Darmstadt, has obtained a CS PhD from Goethe University Frankfurt, and holds a Masters degree in Physics. The main vision behind his research is to transcend static machine learning systems towards adaptive and sustainable lifecycles.</p>
                       </span> 
                    </span>
                 </li>
                 <li id="tutorial6">[15:00-16:00] Tutorial by Tom Rainforth on Modern Bayesian Experimental Design
                    <label for="tutorial6d" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="tutorial6d">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: Bayesian experimental design (BED) provides a powerful and general framework for optimizing the design of experiments. However, its deployment often poses substantial computational challenges that can undermine its practical use. In this tutorial, I will outline the Bayesian experimental design framework and explain how recent advances have transformed our ability to overcome these challenges and thus utilize BED effectively, before discussing some key areas for future development in the field. Related <a href="https://arxiv.org/abs/2302.14545">review paper</a></p>
                          <p style="text-align: justify">Bio: I am a Senior Researcher in Machine Learning (and from September, Associate Professor) in the Department of Statistics at the University of Oxford, where I run the <a href="https://rainml.uk/">RainML Research Lab</a>. My research covers a wide range of topics in and around machine learning and experimental design, with areas of particular interest including Bayesian experimental design, deep learning, representation learning, generative models, Monte Carlo methods, active learning, probabilistic programming, and variational inference.</p>
                       </span> 
                    </span>
                 </li>
    
                 <li>[16:00-16:30] Break</li>
                 <li id="panel1">[16:30-18:00] Panel I: Uncertainty in AI (moderator: Julyan Arbel) <br> 
                  Panelists: Vincent Fortuin, Juho Lee, Matt Jones, Haavard Rue, Eugene Ndiaye, Alexander Immer, Frank Nielsen, Jonghyun Choi, Srijith PK, Marting Mundt, Tom Rainforth 
                </li>
                 
               </ul>
               <p style="text-align: right">[<a href="workshop_2024.html#program">Go to Program</a>]</p>
            </div> 
    </div>
    <div class="w3-row">
        <div class="w3-container w3-padding w3-auto" style="width:100%">
    
            <div id="June17" class="w3-card w3-padding w3-border w3-white w3-round-large">
    
               <h2>June 17</h2>
               <p>Morning Session (chair: Frank Nielsen)</p>
    
               <ul class="w3-ul">
                  <li id="martin">[10:00-11:00] Martin Mundt: Challenging memory as the solution to continual learning
                    <label for="martin1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="martin1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: Deep models tend to fall victim to catastrophic forgetting when updated sequentially or transferred to new scenarios. Continual learning seeks to provide a remedy to this phenomenon. To this end, prevalent approaches construct a relevant memory of the past, typically by retaining what is considered to be most important data. The common assumption is that the solution to continual learning lies in the ability to accumulate prior knowledge. In this talk, I challenge this assumption at the hand of two examples. I will first detail how knowledge can be distilled from a differentiable classifier into a second model, even if we don’t have access to any training data or model internals. Conversely, I will show that models can perform catastrophically in the presence of confounders over time, even if they are equipped with perfect memory of all previously observed data. Together, these examples challenge our present view of memory solutions for continual learning and call out for new techniques to be developed.</p>
                          <p style="text-align: justify">Bio: Martin is an independent research group leader at TU Darmstadt and hessian.AI, where he leads the Open World Lifelong Learning lab. He is also a board member at the non-profit ContinualAI, a core-organizer of Queer in AI, was the Diversity & Inclusion chair at AAAI-24 and currently serves as Review Process Chair for CoLLAs 2024.
                           Previously, he was an interim professor and postdoctoral researcher at TU Darmstadt, has obtained a CS PhD from Goethe University Frankfurt, and holds a Masters degree in Physics. The main vision behind his research is to transcend static machine learning systems towards adaptive and sustainable lifecycles.</p>
                       </span> 
                    </span>
                  </li>
                  
                  <li>[11:00-11:30] Break</li>
    
                  <li id="siddharth">[11:30-12:00] Siddharth Swaroop: Talk TBA
                     <label for="siddharth1" class="read-more-trigger"> Details</label> 
                     <input type="checkbox" class="read-more-state" id="siddharth1">
                     <span class="read-more-wrap">
                        <span class="read-more-target">
                           <p style="text-align: justify">Abstract: TBA</p>
                           <p style="text-align: justify">Bio: TBA</p>
                        </span> 
                     </span>
                  </li>
    
                  <li>[12:30-14:00] Lunch Break (On Your Own) </li>
               </ul>
    
               <p>Afternoon Session</p>
    
               <ul class="w3-ul">
                 <li id="tutorial7">[14:00-15:00] Tutorial by Hossein Mobahi on Optimization for Deep Learning 
                    <label for="tutorial7d" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="tutorial7d">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: TBA</p>
                          <p style="text-align: justify">Bio: TBA</p>
                       </span> 
                    </span>
                 </li>
                 <li id="poster1">[15:00-16:00] Poster Session 1
                    
                 </li>
    
                 <li>[16:00-16:30] Break</li>
    
                 <li id="poster2">[16:30-17:30] Poster Session 2
                    
                 </li>
               </ul>
               <p style="text-align: right">[<a href="workshop_2024.html#program">Go to Program</a>]</p>
            </div> 
    </div>
    <div class="w3-row">
        <div class="w3-container w3-padding w3-auto" style="width:100%">
    
            <div id="June18" class="w3-card w3-padding w3-border w3-white w3-round-large">
    
               <h2>June 18</h2>
               <p>Morning Session (chair: Martin Mundt)</p>
    
               <ul class="w3-ul">
                  <li id="CRESTEmti">[10:00-11:00] CREST Talk - Emtiyaz Khan
                    <label for="CRESTEmti1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="CRESTEmti1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: </p>
                          <p style="text-align: justify">Bio: </p>
                       </span> 
                    </span>
                  </li>
                  
                  <li>[11:00-11:30] Break</li>
    
                  <li id="CRESTThomas">[11:30-12:00] CREST Talk - Thomas Möllenhoff 
                     <label for="CRESTThomas1" class="read-more-trigger"> Details</label> 
                     <input type="checkbox" class="read-more-state" id="CRESTThomas1">
                     <span class="read-more-wrap">
                        <span class="read-more-target">
                           <p style="text-align: justify">Abstract: TBA</p>
                           <p style="text-align: justify">Bio: TBA</p>
                        </span> 
                     </span>
                  </li>
    
                  <li>[12:30-14:00] Lunch Break (On Your Own) </li>
               </ul>
    
               <p>Afternoon Session (chair: Siddharth Swaroop)</p>

               <ul class="w3-ul">
                 <li id="CREST3">[14:00-15:00] CREST Talk 3
                    <label for="CREST3d" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="CREST3d">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: TBA</p>
                          <p style="text-align: justify">Bio: TBA</p>
                       </span> 
                    </span>
                 </li>
                 <li id="CREST4">[15:00-16:00] CREST Talk 4
                    <label for="CREST4d" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="CREST4d">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: TBA</p>
                          <p style="text-align: justify">Bio: TBA</p>
                       </span> 
                    </span>
                 </li>
    
                 <li>[16:00-16:30] Break</li>
    
                 <li id="panel2">[16:30-17:30] Panel II: The future of Lifelong Learning (moderator: Martin Mundt) <br>
                    Panelists: Siddhart Swaroop, Emtiyaz Khan, Thomas Möllenhoff, Hossein Mobahi, Rupam Mahmood,Sarath Chandar,Tom Rainforth, Adam White
                 </li>
                 <li>[18:00-20:00] Speakers Dinner</li>
               </ul>
               <p style="text-align: right">[<a href="workshop_2024.html#program">Go to Program</a>]</p>
            </div> 
    </div>
    <div class="w3-row">
        <div class="w3-container w3-padding w3-auto" style="width:100%">
    
            <div id="June19" class="w3-card w3-padding w3-border w3-white w3-round-large">
    
               <h2>June 19</h2>
               <p>Morning Session (chair: Adam White)</p>
    
               <ul class="w3-ul">
                  <li id="razvan">[10:00-11:00] Razvan Pascanu: Talk TBA
                    <label for="razvan1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="razvan1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: </p>
                          <p style="text-align: justify">Bio: </p>
                       </span> 
                    </span>
                  </li>
                  
                  <li>[11:00-11:30] Break</li>
    
                  <li id="rupam">[11:30-12:00] Rupam Mahmood: Talk TBA
                     <label for="rupam1" class="read-more-trigger"> Details</label> 
                     <input type="checkbox" class="read-more-state" id="rupam1">
                     <span class="read-more-wrap">
                        <span class="read-more-target">
                           <p style="text-align: justify">Abstract: TBA</p>
                           <p style="text-align: justify">Bio: TBA</p>
                        </span> 
                     </span>
                  </li>
    
                  <li>[12:30-14:00] Lunch Break (On Your Own) </li>
               </ul>
    
               <p>Afternoon Session (chair: Razvan Pascanu)</p>

               <ul class="w3-ul">
                 <li id="sarath">[14:00-15:00] Sarath Chandar: Talk TBA
                    <label for="sarath1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="sarath1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: TBA</p>
                          <p style="text-align: justify">Bio: TBA</p>
                       </span> 
                    </span>
                 </li>
                 <li id="tom">[15:00-16:00] Tom Rainforth: Do Bayesian Neural Networks Need To Be Fully Stochastic?
                    <label for="tom1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="tom1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: In this talk I will discuss whether the standard assumption of treating all the parameters in a Bayesian neural network stochastically is actually justified.  I walk explain how our recent work has given compelling theoretical and empirical evidence that this standard construction may be unnecessary.  From a theoretical perspective, I will demonstrate that expressive predictive distributions require only small amounts of stochasticity.  From an empirical perspective, I will explain how our investigations found no systematic benefit of full stochasticity across four different inference modalities and eight datasets. I will try to leave plenty of time at the end of the talk for debate on the subject!</p>
                          <p style="text-align: justify">Bio: I am a Senior Researcher in Machine Learning (and from September, Associate Professor) in the Department of Statistics at the University of Oxford, where I run the <a href="https://rainml.uk/">RainML Research Lab</a>. My research covers a wide range of topics in and around machine learning and experimental design, with areas of particular interest including Bayesian experimental design, deep learning, representation learning, generative models, Monte Carlo methods, active learning, probabilistic programming, and variational inference.</p>
                       </span> 
                    </span>
                 </li>
    
                 <li>[16:00-16:30] Break</li>
    
                 <li id="adam">[16:30-17:30] Adam White: Continual Subtask Learning
                    <label for="adam1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="adam1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: In many real-world problems the agent is much smaller than the vast world in which it must operate. In such scenarios, the world appears non-stationary to the agent, and thus we require agents capable of stable, non-convergent, never-ending learning. Successful agents must balance specializing their learning to the current situation with the need to learn many things over time which can be combined to learn yet new things--a concept known as scaffolding. This talk will review my lab's recent work on architectures and algorithms for learning many things.</p>
                          <p style="text-align: justify">Bio: Adam is an assistant professor in the University of Alberta’s Department of Computing Science, a Canada CIFAR AI Chair, and the Director of Scientific Operations at the Alberta Machine Intelligence Institute (Amii).  He is also a Principal Investigator in the Reinforcement Learning & Artificial Intelligence (RLAI) Lab. Adam co-created the Reinforcement Learning Specialization, taken by over 85,000 students on Coursera. Adam's research is focused on understanding the fundamental principles of learning in continual learning settings, both simulated worlds and real-world industrial-control applications. Adam's group is deeply passionate about good empirical practices and new methodologies to help determine if our algorithms are ready for deployment in the real world.</p>
                       </span> 
                    </span>
                 </li>
                 
               </ul>
               <p style="text-align: right">[<a href="workshop_2024.html#program">Go to Program</a>]</p>
            </div> 
    </div>
    <div class="w3-row">
        <div class="w3-container w3-padding w3-auto" style="width:100%">
    
            <div id="June20" class="w3-card w3-padding w3-border w3-white w3-round-large">
    
               <h2>June 20</h2>
               <p>Morning Session (chair: Adam White)</p>
    
               <ul class="w3-ul">
                  <li id="hossein">[10:00-11:00] Hossein Mobahi: Neglected Hessian component explains mysteries in Sharpness regularization
                    <label for="hossein1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="hossein1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: Recent work has shown that methods like SAM which either explicitly or implicitly penalize second order information can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We show that these differences can be explained by the structure of the Hessian of the loss. First, we show that a common decomposition of the Hessian can be quantitatively interpreted as separating the feature exploitation from feature exploration. The feature exploration, which can be described by the Nonlinear Modeling Error matrix (NME), is commonly neglected in the literature since it vanishes at interpolation. Our work shows that the NME is in fact important as it can explain why gradient penalties are sensitive to the choice of activation function. Using this insight we design interventions to improve performance. We also provide evidence that challenges the long held equivalence of weight noise and gradient penalties. This equivalence relies on the assumption that the NME can be ignored, which we find does not hold for modern networks since they involve significant feature learning. We find that regularizing feature exploitation but not feature exploration yields performance similar to gradient penalties.</p>
                          <p style="text-align: justify">Bio: Hossein Mobahi is a senior research scientist at Google DeepMind, where his interests focus on the intersection of optimization and generalization in deep neural networks. Before joining Google in 2016, he was a postdoctoral researcher at MIT's CSAIL. He holds a PhD in Computer Science from the University of Illinois at Urbana-Champaign (UIUC).</p>
                       </span> 
                    </span>
                  </li>
                  
                  <li>[11:00-11:30] Break</li>
    
                  <li id="arindam">[11:30-12:00] Arindam Banerjee: Talk TBA
                     <label for="arindam1" class="read-more-trigger"> Details</label> 
                     <input type="checkbox" class="read-more-state" id="arindam1">
                     <span class="read-more-wrap">
                        <span class="read-more-target">
                           <p style="text-align: justify">Abstract: TBA</p>
                           <p style="text-align: justify">Bio: TBA</p>
                        </span> 
                     </span>
                  </li>
    
                  <li>[12:30-14:00] Lunch Break (On Your Own) </li>
               </ul>
    
               <p>Afternoon Session</p> 
               <ul class="w3-ul">
                 <li id="poster3">[14:00-15:00] Poster Session 3
                 </li>
                 <li id="poster4">[15:00-16:00] Poster Session 4
                 </li>
    
                 <li>[16:00-16:30] Break</li>
    
                 <li id="poster5">[16:30-17:30] Poster Session 5
                 </li>
                 
               </ul>
               <p style="text-align: right">[<a href="workshop_2024.html#program">Go to Program</a>]</p>
            </div> 
    </div>
    <div class="w3-row">
        <div class="w3-container w3-padding w3-auto" style="width:100%">
    
            <div id="June21" class="w3-card w3-padding w3-border w3-white w3-round-large">
    
               <h2>June 21</h2>
               <p>Morning Session (chair: Zelda Mariet)</p>
    
               <ul class="w3-ul">
                  <li id="yingzhen">[10:00-11:00] Yingzhen Li: Towards Causal Deep Generative Models for Sequential Data
                    <label for="yingzhen1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="yingzhen1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: One of my research dreams is to build a high-resolution video generation model that enables granularity controls in e.g., the scene appearance and the interactions between objects. I tried, and then realised the need of me inventing deep learning tricks for this goal is due to the issue of non-identifiability in my sequential deep generative models. In this talk I will discuss our research towards developing identifiable deep generative models in sequence modelling, and share some recent and on-going works regarding switching dynamic models. Throughout the talk I will highlight the balance between causality "Theorist" and deep learning "Alchemist", and discuss my opinions on the future of causal deep generative modelling research.</p>
                          <p style="text-align: justify">Bio: Dr Yingzhen Li is a Senior Lecturer in Machine Learning at Imperial College London, UK. Before that she worked at Microsoft Research Cambridge and Disney Research. She received her PhD from the University of Cambridge. Yingzhen is passionate about building reliable machine learning systems with probabilistic methods, and her published work has been applied in industrial systems and implemented in popular deep learning frameworks. Her work on Bayesian ML has been recognised in AAAI 2023 New Faculty Highlights, and she gave an invited tutorial on approximate inference at NeurIPS 2020. She regularly serves as (Senior) Area Chair for ICML, ICLR and NeurIPS, and she is a Program Chair for AISTATS 2024. When not at work, Yingzhen enjoys reading, hiking, video games, and following news on latest technology developments.</p>
                       </span> 
                    </span>
                  </li>
                  
                  <li>[11:00-11:30] Break</li>
    
                  <li id="daiki">[11:30-12:00] Daiki Chijiwa: Talk TBA
                     <label for="daiki1" class="read-more-trigger"> Details</label> 
                     <input type="checkbox" class="read-more-state" id="daiki1">
                     <span class="read-more-wrap">
                        <span class="read-more-target">
                           <p style="text-align: justify">Abstract: TBA</p>
                           <p style="text-align: justify">Bio: TBA</p>
                        </span> 
                     </span>
                  </li>
    
                  <li>[12:30-14:00] Lunch Break (On Your Own) </li>
               </ul>
    
               <p>Afternoon Session (chair: Yingzhen Li)</p>
               <ul class="w3-ul">
                <li id="zelda">[14:00-15:00] Zelda Mariet: Talk TBA
                    <label for="zelda1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="zelda1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: TBA</p>
                          <p style="text-align: justify">Bio: TBA</p>
                       </span> 
                    </span>
                 </li>
                 
                 <li>[15:00-16:10] Break</li>

                 <li id="ehsan">[16:10-17:10] Ehsan Amid: Understanding and Improving Representation and Memory in Deep Neural Networks
                    <label for="ehsan1" class="read-more-trigger"> Details</label> 
                    <input type="checkbox" class="read-more-state" id="ehsan1">
                    <span class="read-more-wrap">
                       <span class="read-more-target">
                          <p style="text-align: justify">Abstract: We first provide insights into how modern neural networks, such as transformers, learn representations and incorporate a memory mechanism in their architecture. Next, we present a general approach to learning such representations from a trained network and transferring those representations into a second network with a different size or architecture. Specifically, our strategy relies on a non-linear variant of PCA via Bregman divergences. Finally, we discuss some possible future directions for enhancing representation and memory in deep neural networks.</p>
                          <p style="text-align: justify">Bio: Ehsan Amid is a Research Scientist at Google DeepMind (formerly Google Brain). He received his PhD in Computer Science (with a focus on Machine Learning) from the University of California, Santa Cruz, and an MSc degree in Machine Learning and Data Mining from Aalto University, Finland. He works on machine learning theory, robust learning, optimization, and dimensionality reduction techniques. He is a Gemini core contributor for training multimodal large language models.</p>
                       </span> 
                    </span>
                 </li>
    
                 
    
                 <li id="panel3">[17:10-18:40] Panel III: The future of AI (moderator: Emtiyaz Khan)
                    <br> Panelists: Arindam Banerjee, Yingzhen Li, Daiki Chijiwa, Zelda Mariet, Ehsan Amid, Kennichi Bannai, Razvan Pascanu 
                 </li>
                 
               </ul>
               <p style="text-align: right">[<a href="workshop_2024.html#program">Go to Program</a>]</p>
            </div> 
    </div>
</div>
  
</body>
</html>
